---
title: "Run MOMAM - short ver."
author: "Michael Wiley"
date: "2023-12-03"
output: pdf_document
---

## The short version removes the data extraction and database building steps (~10 mins)
## as well as greatly reducing the number of models trained (~1 hour saved)

## Step 0: Setup

```{r libraries, echo = FALSE, message = FALSE, warning = FALSE}
library(stringr)
library(dplyr)
library(magrittr)
library(readr)
library(httr)
library(jsonlite)
library(RSQLite)
library(tidyr)
library(data.table)
library(purrr)
library(stringi)
library(stringr)
library(lubridate)
library(here)
library(tidymodels)
library(fastDummies)
library(yardstick)
library(discrim)
library(sda)
library(finetune)
library(doParallel)
tidymodels_prefer()
```

Load all helper functions located in `lib`.
```{r load_helper_functions, echo = FALSE, message = FALSE, warning = FALSE}
source_files <- list.files(here("lib")) %>%
  str_subset("\\.R$") %>%
  str_c(here("lib"), "/", .)

lapply(source_files, source)
```
Various global environment variable setup:

Establish database connection
Load all csv files
Creates tables name
Loads API information
```{r gloabl_var_setup}
#db <- dbConnect(SQLite(), str_c(here("output"), "/", "MOMAM.sqlite"))
db <- connect_db(here("output"), "MOMAM.sqlite")
csvs <- load_all_csvs_from_folder(here("data/channel_data/"))

tablenames <- str_c(tolower(str_extract(csvs, "(pie|Spike)")),
                    str_extract(csvs, "20[:number:]+"))


# End points with necessary information from the API.
# TO DO: Organize this into a single data structure.
api_url <- "https://api.igdb.com/v4"
end_games <- "/games"
end_categories <- "/categories"
end_genres <- "/genres"
end_age <- "/age_ratings"
end_company <- "/involved_companies"
end_engines <- "/game_engines"
end_platforms <- "/platforms"
end_franchises <- "/franchises"
end_themes <- "/themes"
end_keywords <- "/keywords"
end_perspectives <- "/player_perspectives"

client_id <- get_client_id()
access_token <- get_access_token()

```


## Step 2: Prepare Data for Analysis

The above step steps collected the necessary data and placed it into normalized SQL tables

Now it's time to move that data into a analysis-ready format.

This cell here will connect to the database containing our game metadata, as well
the csv file holding the results of all previous MOMAMs.

game_list is our master list of games that the streamers have played
cumulative_playtime is our list of how long each streamer spent playing each game in total
imputed_yearly_metadata lists pertinent metadata for each game, such as genre and release data.

```{r setup}
#db <- connect_db(here("output"), "MOMAM.sqlite")
past_results <- read_csv(str_c(here("data/momam7"), "/momam7_base_data.csv"))

game_list <- dbGetQuery(db, 'SELECT game_id, game FROM game_data;') %>%
  as_tibble()

cumulative_playtime <- dbGetQuery(db, 'SELECT * FROM cumulative_playtime;') %>%
  as_tibble()

imputed_yearly_metadata <- dbGetQuery(db, 'SELECT * FROM imputed_yearly_metadata') %>%
  as_tibble()
```

The following cells will all focus around taking our db normalized data and preparing it for
analysis. The first thing to do is create dummy variables columns for our categorical variables,
but not our numeric ones.

```{r categorical_dummies}
past_results_with_id <- past_results %>%
  left_join(game_list, by = join_by(game)) %>%
  left_join(imputed_yearly_metadata, by = join_by(game_id == id), relationship = "many-to-many")

past_results_categorical <- past_results_with_id %>%
  dplyr::filter(!(name %in% c("first_release_date", "total_rating"))) %>%
  distinct() %>%
  pivot_wider(names_from = c(name, value), values_from = value,
              names_glue = "{name}_{value}") %>%
  mutate(across(-c(MOMAM, grouping, game, category, winner, game_id),
                ~ ifelse(is.na(.), 0, 1)))

past_results_numeric <- past_results_with_id %>%
  dplyr::filter((name %in% c("first_release_date", "total_rating"))) %>%
  distinct() %>%
  pivot_wider(names_from = name, values_from = value, values_fn = mean)

# For reordering columns with the pivoted columns in the back.
desired_order <- c("winner", "MOMAM", "grouping", "game", "category", "game_id")

# Combines the categorical and numeric dfs.
past_results_pivoted_metadata <- past_results_categorical %>%
  left_join(past_results_numeric, by = join_by(MOMAM, grouping, game, category, game_id, winner)) %>%
  dplyr::select(sort(names(.))) %>%
  dplyr::select(all_of(desired_order), everything()) %>%
  dplyr::select(-category) # category is handled by the cumulative time function below.
```

Next is to deal with multi-game challenges. Sometimes one day will consist of a race of 2 or more games and we need to treat these days one prediction. To achieve this, each game that's played on the same day as the data in all of their columns averaged out. The comments in the code should have more detail.

```{r group_counts}
# Counts how many games are in each group.
# A group is a set of games that either are being played or can be played on the same day.
# For example, if there's a bid war between Ocarina of Time and Majora's Mask,
# they would share the same grouping for that entry.
counts <- past_results %>%
  group_by(grouping) %>%
  count(grouping)

# Splits games with multiple categories and assigns weights to each entry depending
# on their grouping. All of the joins should go here.
past_results_pivoted <- past_results %>%
  left_join(counts, by = join_by(grouping)) %>%
  mutate(weight = 1/n) %>%
  dplyr::select(-n) %>%
  left_join(game_list, by = join_by(game)) %>%
  left_join(cumulative_playtime, by = join_by(game_id), multiple = "all", relationship = "many-to-many") %>%
  dummy_cols(select_columns = c("category"),
             remove_selected_columns = T,
             split = ",") %>%
  pivot_wider(names_from = streamer, values_from = minutes, values_fill = 0,
              names_glue = "cum_playtime_{streamer}") %>%
  left_join(past_results_pivoted_metadata, by = join_by(MOMAM, grouping, game, winner, game_id))


# Sets the data up to be weighted. Will lose game id with this, so need to have all the data
# ready to go by this point.
past_results_weighted <- past_results_pivoted %>%
  pivot_longer(cols = c(starts_with("category"), starts_with("cum"),
                        starts_with("first"), starts_with("franchise"),
                        starts_with("genre"), starts_with("involved"),
                        starts_with("platform"), starts_with("player"),
                        starts_with("theme"), starts_with("total")), # Brings all of the columns together
               names_to = "placeholder") %>%                        # that need to be weighted.
  mutate(weighted_val = value * weight) %>% # Applies the weight
  group_by(MOMAM, grouping, winner, placeholder) %>%
  summarise(weighted_group = sum(weighted_val)) %>%
  pivot_wider(names_from = placeholder, values_from = weighted_group, values_fill = 0) # Puts the data back in their dummy column
```

```{r check_head}
head(past_results_weighted)
```
Note that this is problematic here - there are significantly more columns than rows at the moment.
```{r check_dim}
dim(past_results_weighted)
```

## Step 3: Analysis

Now that the data has been created, it's time to analyze it.

First up, a preprocessing workflow is created.
Since we have so many columns - many of which have near-zero variance, we will
filter those out.

```{r train-test-split}
past_results_weighted$winner <- as.factor(past_results_weighted$winner)

set.seed(323)

# Ideally the size of the test set will be the size of the last MOMAM. 
target_split <- 1 -
  nrow(past_results_weighted[past_results_weighted$MOMAM == 7, ]) /
  nrow(past_results_weighted)

initial_split <- initial_split(past_results_weighted,
              prop = target_split,
              strata = MOMAM)

momam_train <- training(initial_split)
momam_test <- testing(initial_split)

momam_folds <- vfold_cv(momam_train)
```

```{r pre-process}
base_recipe <- recipe(winner ~ ., data = momam_train) %>%
  step_rm(grouping) %>%
  step_mutate(cum_playtime_pie = log(cum_playtime_pie + 1)) %>%
  step_mutate(cum_playtime_spike = log(cum_playtime_spike + 1)) %>%
  step_naomit(c(cum_playtime_pie, cum_playtime_spike)) %>%
  step_nzv(all_predictors(), freq_cut = 0.01, unique_cut = 0) %>%
  step_zv(all_predictors()) %>%
  step_lincomb(all_numeric_predictors()) %>%
  step_corr(all_predictors()) %>%
  step_normalize(all_predictors())

# An opinionated way of reducing the dimensionality. This will create different
# PCs for each group of variables.
# However, different thresholds are used for each of categories because domain
# knowledge tells me that some of these items are significantly more important
# than others.
# For example, enough PCs are taken from genre columns in order to explain
# at least 90% of the variance because genre is one of the most important factors
# for who will end up winning.
grouped_pca <- recipe(winner ~ ., data = momam_train) %>%
  step_rm(grouping) %>%
  step_mutate(cum_playtime_pie = log(cum_playtime_pie + 1)) %>%
  step_mutate(cum_playtime_spike = log(cum_playtime_spike + 1)) %>%
  step_naomit(c(cum_playtime_pie, cum_playtime_spike)) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pca(starts_with("category"), threshold = 0.5, prefix = "category_") %>%
  step_pca(starts_with("cum"), threshold = 0.8, prefix = "cum") %>%
  step_pca(starts_with("franchises"), threshold = 0.2, prefix = "franchises") %>%
  step_pca(starts_with("genres"), threshold = 0.9, prefix = "genres") %>%
  step_pca(starts_with("platforms"), threshold = 0.80, prefix = "platforms") %>%
  step_pca(starts_with("player"), threshold = 0.8, prefix = "player") %>%
  step_pca(starts_with("themes"), threshold = 0.8, prefix = "themes") %>%
  step_pca(starts_with("involved"), threshold = 0.2, prefix = "involved") %>%
  step_corr(all_predictors(), threshold = 0.9)

# Preforms an additional PCA on the grouped PCA since the dimensionality produced
# by that approach is still rather high.
# this processor ends up being one of the most effective after training.
hierarchical_pca <- grouped_pca %>%
  step_pca(-c("MOMAM", "first_release_date", "total_rating", "winner"),
           threshold = 0.8, prefix = "hier_")

pca_strict_recipe <- base_recipe %>%
  step_pca(all_predictors(), threshold = 0.8)
  
```


Define models:

These are the models is trained on. Currently a few simple models are here as a baseline,
but ideally more will be used.

```{r models}
model_dt <- decision_tree(tree_depth = tune(), min_n = tune(), cost_complexity = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  translate()

model_rf <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("randomForest") %>% 
  set_mode("classification") %>% 
  translate()

model_xgb <- boost_tree(tree_depth = tune(),
                        trees = tune(),
                        learn_rate = tune(),
                        mtry = tune(),
                        min_n = tune(),
                        loss_reduction = tune(),
                        sample_size = tune(),
                        stop_iter = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification") %>%
  translate()
```

List of models to train. Each model will be trained with each preprocessing technique.

```{r workflow}
momam_workflowset <- workflow_set(
  preproc = list(base = base_recipe,
                 strict = pca_strict_recipe,
                 hierarchical_pca = hierarchical_pca),
  models = list(
    decision_tree = model_dt,
    rand_forest = model_rf,
    xgboost = model_xgb
  ),
  cross = TRUE
)
```

Trains and tunes each model with each pre-processor.

```{r train}
race_ctrl <-
  control_race(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

library(doParallel)
doParallel::registerDoParallel(cores = 10)

grid_results <-
  momam_workflowset %>%
  workflow_map(
    "tune_race_anova",
    seed = 0323,
    resamples = momam_folds,
    grid = 15,
    control = race_ctrl,
    verbose = TRUE
  )

doParallel::stopImplicitCluster()
```

Displays how the models performed.
Accuracy was chosen as the metric since we care more about predicting the winner
correctly by any means necessary moreso than which predictions the model is
getting right or wrong.

```{r evaluate}
autoplot(
  grid_results,
  rank_metric = "accuracy",  # <- how to order models
  #metric = "accuracy",       # <- which metric to visualize
  select_best = TRUE     # <- one point per workflow
)
```

Ranks each of the trained models.

```{r model_rankings}
grid_results %>% 
  workflowsets::rank_results(rank_metric = "accuracy") %>%
  pivot_wider(names_from = .metric, values_from = mean) %>%
  group_by(.config) %>% 
  fill(accuracy, roc_auc, .direction = "downup") %>%
  distinct(rank, .keep_all = TRUE) %>%
  mutate(preprocessor = str_extract(wflow_id, ".*?(?=_)")) %>%
  select(wflow_id, model, preprocessor, accuracy, roc_auc, rank) %>%
  arrange(desc(roc_auc))
```

View the tuning parameters of the best model.

```{r best_parameters}
best_results <- 
  grid_results %>% 
  extract_workflow_set_result("hierarchical_pca_rand_forest") %>% 
  select_best(metric = "accuracy")

best_results
```

Extract the best model and see how it performs on the test set.

```{r best_extract}
best_test_results <- 
  grid_results %>% 
  extract_workflow("hierarchical_pca_rand_forest") %>% 
  finalize_workflow(best_results) %>% 
  last_fit(split = initial_split)

best_test_results$.metrics
```

View individual predictions of the best model on the test set.

```{r test_predictions}
predictions <- best_test_results %>% 
  collect_predictions() 

predictions

predictions %>%
  mutate(correct = ifelse(.pred_class == winner, 1, 0)) %>%
  mutate(weighted_answer = case_when(winner == "pie" ~ .pred_pie,
                                     T ~ .pred_spike)) %>%
  summarise(test_acc = sum(correct)/nrow(.), test_roc = sum(weighted_answer)/nrow(.))
```