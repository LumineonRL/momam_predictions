---
title: "Run MOMAM"
author: "Michael Wiley"
date: "2023-12-03"
output: pdf_document
---

## Step 0: Setup

```{r libraries, echo = FALSE, message = FALSE, warning = FALSE}
library(stringr)
library(dplyr)
library(magrittr)
library(readr)
library(httr)
library(jsonlite)
library(RSQLite)
library(tidyr)
library(data.table)
library(purrr)
library(stringi)
library(stringr)
library(lubridate)
library(here)
library(tidymodels)
library(fastDummies)
library(yardstick)
library(discrim)
library(sda)
tidymodels_prefer()
```

Load all helper functions located in `lib`.
```{r load_helper_functions, echo = FALSE, message = FALSE, warning = FALSE}
source_files <- list.files(here("lib")) %>%
  str_subset("\\.R$") %>%
  str_c(here("lib"), "/", .)

lapply(source_files, source)
```
Various global environment variable setup:

Establish database connection
Load all csv files
Creates tables name
Loads API information
```{r gloabl_var_setup}
db <- connect_db(here("output"), "MOMAM.sqlite")
csvs <- load_all_csvs_from_folder(here("data/channel_data/"))

tablenames <- str_c(tolower(str_extract(csvs, "(pie|Spike)")),
                    str_extract(csvs, "20[:number:]+"))


# End points with necessary information from the API.
# TO DO: Organize this into a single data structure.
api_url <- "https://api.igdb.com/v4"
end_games <- "/games"
end_categories <- "/categories"
end_genres <- "/genres"
end_age <- "/age_ratings"
end_company <- "/involved_companies"
end_engines <- "/game_engines"
end_platforms <- "/platforms"
end_franchises <- "/franchises"
end_themes <- "/themes"
end_keywords <- "/keywords"
end_perspectives <- "/player_perspectives"

client_id <- get_client_id()
access_token <- get_access_token()

```

## Step 1: Obtain Data
**WARNING: This cell can take ~10 minutes to run.**
**You can skip Step 1 entirely and move straight to Step 2 if you are not attempting to**
**recreate the `output/MOMAM.sqlite` database from scratch.**

- Loads in the annual play time data for each game each streamer has played each year
- Searches the game in vgdb to obtain the game's id number to use to later obtain metadata on each game
- Writes the data to the database.

- Please refer to the functions in `lib/twitch_game_and_channel_data` for further documenation.

One *only* needs to run this cell if trying to reproduce `output/MOMAM.sqlite` from scratch.
Running it with an existing `MOMAM.sqlite` files with no new csvs in `data/channel_data/` will
attempt to merge the exact same data into the same tables in the db but end up doing nothing because
the data already exists.

*In other words* if this cell is taking too long to run, simply skip it and everything else
will run fine provided the above paragraph is true.
```{r process_csvs}
lapply(csvs, function(csv) {
  construct_yearly_time(db, csv)
})

dbExecute(db, "DROP VIEW IF EXISTS cumulative_playtime;")
dbExecute(db, "CREATE VIEW IF NOT EXISTS cumulative_playtime AS
           SELECT streamer, game_id, SUM(minutes) AS minutes
           FROM yearly_playtime
           GROUP BY streamer, game_id;")
```

```{r db_check}
dbGetQuery(db, "SELECT * FROM cumulative_playtime;")
```

The csv files contained data on every game the two streamers have played between 2016 and 2022
and we took all of those games, and added an entry in our database for them.

However, what about games they haven't played? Every year MOMAM features games that neither
participant has played before and is thus not in the csvs with their past data.

The below manually adds entries for games that will be played in an upcoming MOMAM
so that their metadata can be collected as well.
```{r manual_additions}
manual_db_additions(db)
```

The following function will:
1) Take every game pie and spike have ever streamed and construct a query that
will grab any potentially useful meta data from that query
2) Send the query to the API
3) Parse and normalize the result of the query
4) Write the query to the MOMAM database.

Please view `lib/get_game_metadata.R` for more detailed documentation.

```{r collect_metadata}
games_run_query(db)
```

Creates aggregate views of the data in order to know how much time each streamer has spent
playing games with a certain piece of metadata.

- `imputed_yearly_metadata` is an intermediary table used to remove null values when calculating
play time for the next two view
- `yearly_metadata_playtime` lists how long each streamer spent playing games with particular
metadata each year.
- `cumulative_metadata_playtime` lists how long each streamer played games with particular metadata
throughout their streaming career.

The yearly data is actually very important. Imagine if streamer A has 100 total hours on games in the
`action` genre, but 95 of those hours were in 2022. Since our model will be learning from past MOMAMs
that have been happening since 2016, it's important to have the context of what their experience with the
genre was *at the time* and not necessarily what their current, cumulative experience with it is.
```{r create_views}
# I wish SQLite had 'CREATE OR REPLACE' statements.
dbExecute(db, "DROP VIEW IF EXISTS imputed_yearly_metadata;")
dbExecute(db, "DROP VIEW IF EXISTS yearly_metadata_playtime;")
dbExecute(db, "DROP VIEW IF EXISTS cumulative_metadata_playtime;")

# Some games don't have a release date or rating in vgdb, so this view imputes one from the average.
dbExecute(db, "CREATE VIEW IF NOT EXISTS imputed_yearly_metadata AS
            SELECT DISTINCT id, name,
            ROUND(COALESCE(value, AVG(value) OVER(PARTITION BY name)), 0) value
            FROM games_metadata")


dbExecute(db, "CREATE VIEW IF NOT EXISTS yearly_metadata_playtime AS
           SELECT y.streamer, y.year, g.name, g.value, SUM(y.minutes) minutes
           FROM yearly_playtime y
           LEFT JOIN imputed_yearly_metadata g ON y.game_id = g.id
           GROUP BY y.streamer, y.year, g.name, g.value")


dbExecute(db, "CREATE VIEW IF NOT EXISTS cumulative_metadata_playtime AS
           SELECT streamer, name, value, SUM(minutes) minutes
           FROM yearly_metadata_playtime
           GROUP BY streamer, name, value;")
```
```{r db_disconnect}
dbDisconnect(db)
```


## Step 2: Prepare Data for Analysis

The above step steps collected the necessary data and placed it into normalized SQL tables

Now it's time to move that data into a analysis-ready format.

This cell here will connect to the database containing our game metadata, as well
the csv file holding the results of all previous MOMAMs.

game_list is our master list of games that the streamers have played
cumulative_playtime is our list of how long each streamer spent playing each game in total
imputed_yearly_metadata lists pertinent metadata for each game, such as genre and release data.

```{r setup}
db <- connect_db(here("output"), "MOMAM.sqlite")
past_results <- read_csv(str_c(here("data/momam7"), "/momam7_base_data.csv"))

game_list <- dbGetQuery(db, 'SELECT game_id, game FROM game_data;') %>%
  as_tibble()

cumulative_playtime <- dbGetQuery(db, 'SELECT * FROM cumulative_playtime;') %>%
  as_tibble()

imputed_yearly_metadata <- dbGetQuery(db, 'SELECT * FROM imputed_yearly_metadata') %>%
  as_tibble()
```

The following cells will all focus around taking our db normalized data and preparing it for
analysis. The first thing to do is create dummy variables columns for our categorical variables,
but not our numeric ones.

```{r categorical_dummies}
past_results_with_id <- past_results %>%
  left_join(game_list, by = join_by(game)) %>%
  left_join(imputed_yearly_metadata, by = join_by(game_id == id), relationship = "many-to-many")

past_results_categorical <- past_results_with_id %>%
  dplyr::filter(!(name %in% c("first_release_date", "total_rating"))) %>%
  distinct() %>%
  pivot_wider(names_from = c(name, value), values_from = value,
              names_glue = "{name}_{value}") %>%
  mutate(across(-c(MOMAM, grouping, game, category, winner, game_id),
                ~ ifelse(is.na(.), 0, 1)))

past_results_numeric <- past_results_with_id %>%
  dplyr::filter((name %in% c("first_release_date", "total_rating"))) %>%
  distinct() %>%
  pivot_wider(names_from = name, values_from = value, values_fn = mean)

# For reordering columns with the pivoted columns in the back.
desired_order <- c("winner", "MOMAM", "grouping", "game", "category", "game_id")

# Combines the categorical and numeric dfs.
past_results_pivoted_metadata <- past_results_categorical %>%
  left_join(past_results_numeric, by = join_by(MOMAM, grouping, game, category, game_id, winner)) %>%
  dplyr::select(sort(names(.))) %>%
  dplyr::select(all_of(desired_order), everything()) %>%
  dplyr::select(-category) # category is handled by the cumulative time function below.
```

Next is to deal with multi-game challenges. Sometimes one day will consist of a race of 2 or more games and we need to treat these days one prediction. To achieve this, each game that's played on the same day as the data in all of their columns averaged out. The comments in the code should have more detail.

```{r group_counts}
# Counts how many games are in each group.
# A group is a set of games that either are being played or can be played on the same day.
# For example, if there's a bid war between Ocarina of Time and Majora's Mask,
# they would share the same grouping for that entry.
counts <- past_results %>%
  group_by(grouping) %>%
  count(grouping)

# Splits games with multiple categories and assigns weights to each entry depending
# on their grouping. All of the joins should go here.
past_results_pivoted <- past_results %>%
  left_join(counts, by = join_by(grouping)) %>%
  mutate(weight = 1/n) %>%
  dplyr::select(-n) %>%
  left_join(game_list, by = join_by(game)) %>%
  left_join(cumulative_playtime, by = join_by(game_id), multiple = "all", relationship = "many-to-many") %>%
  dummy_cols(select_columns = c("category"),
             remove_selected_columns = T,
             split = ",") %>%
  pivot_wider(names_from = streamer, values_from = minutes, values_fill = 0,
              names_glue = "cum_playtime_{streamer}") %>%
  left_join(past_results_pivoted_metadata, by = join_by(MOMAM, grouping, game, winner, game_id))


# Sets the data up to be weighted. Will lose game id with this, so need to have all the data
# ready to go by this point.
past_results_weighted <- past_results_pivoted %>%
  pivot_longer(cols = c(starts_with("category"), starts_with("cum"),
                        starts_with("first"), starts_with("franchise"),
                        starts_with("genre"), starts_with("involved"),
                        starts_with("platform"), starts_with("player"),
                        starts_with("theme"), starts_with("total")), # Brings all of the columns together
               names_to = "placeholder") %>%                        # that need to be weighted.
  mutate(weighted_val = value * weight) %>% # Applies the weight
  group_by(MOMAM, grouping, winner, placeholder) %>%
  summarise(weighted_group = sum(weighted_val)) %>%
  pivot_wider(names_from = placeholder, values_from = weighted_group, values_fill = 0) # Puts the data back in their dummy column
```

```{r check_head}
head(past_results_weighted)
```
Note that this is problematic here - there are significantly more columns than rows at the moment.
```{r check_dim}
dim(past_results_weighted)
```

## Step 3: Analysis

Now that the data has been created, it's time to analyze it.

First up, a preprocessing workflow is created.
Since we have so many columns - many of which have near-zero variance, we will
filter those out.

```{r train-test-split}
past_results_weighted$winner <- as.factor(past_results_weighted$winner)

set.seed(323)

# Ideally the size of the test set will be the size of the last MOMAM. 
target_split <- 1 -
  nrow(past_results_weighted[past_results_weighted$MOMAM == 7, ]) /
  nrow(past_results_weighted)

initial_split <- initial_split(past_results_weighted,
              prop = target_split,
              strata = MOMAM)

momam_train <- training(initial_split)
momam_test <- testing(initial_split)

momam_folds <- vfold_cv(momam_train)
```

```{r pre-process}
base_recipe <- recipe(winner ~ ., data = momam_train) %>%
  step_rm(grouping) %>%
  step_mutate(cum_playtime_pie = log(cum_playtime_pie + 1)) %>%
  step_mutate(cum_playtime_spike = log(cum_playtime_spike + 1)) %>%
  step_naomit(c(cum_playtime_pie, cum_playtime_spike)) %>%
  step_nzv(all_predictors(), freq_cut = 0.01, unique_cut = 0) %>%
  step_zv(all_predictors()) %>%
  step_lincomb(all_numeric_predictors()) %>%
  step_corr(all_predictors()) %>%
  step_normalize(all_predictors())

# An opinionated way of reducing the dimensionality. This will create different
# PCs for each group of variables.
# However, different thresholds are used for each of categories because domain
# knowledge tells me that some of these items are significantly more important
# than others.
# For example, enough PCs are taken from genre columns in order to explain
# at least 90% of the variance because genre is one of the most important factors
# for who will end up winning.
grouped_pca <- recipe(winner ~ ., data = momam_train) %>%
  step_rm(grouping) %>%
  step_mutate(cum_playtime_pie = log(cum_playtime_pie + 1)) %>%
  step_mutate(cum_playtime_spike = log(cum_playtime_spike + 1)) %>%
  step_naomit(c(cum_playtime_pie, cum_playtime_spike)) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pca(starts_with("category"), threshold = 0.5, prefix = "category_") %>%
  step_pca(starts_with("cum"), threshold = 0.8, prefix = "cum") %>%
  step_pca(starts_with("franchises"), threshold = 0.2, prefix = "franchises") %>%
  step_pca(starts_with("genres"), threshold = 0.9, prefix = "genres") %>%
  step_pca(starts_with("platforms"), threshold = 0.80, prefix = "platforms") %>%
  step_pca(starts_with("player"), threshold = 0.8, prefix = "player") %>%
  step_pca(starts_with("themes"), threshold = 0.8, prefix = "themes") %>%
  step_pca(starts_with("involved"), threshold = 0.2, prefix = "involved") %>%
  step_corr(all_predictors(), threshold = 0.9)

# Preforms an additional PCA on the grouped PCA since the dimensionality produced
# by that approach is still rather high.
# this processor ends up being one of the most effective after training.
hierarchical_pca <- grouped_pca %>%
  step_pca(-c("MOMAM", "first_release_date", "total_rating", "winner"),
           threshold = 0.8, prefix = "hier_")

pca_recipe <- base_recipe %>%
  step_pca(all_predictors(), threshold = 0.2)

pca_strict_recipe <- base_recipe %>%
  step_pca(all_predictors(), threshold = 0.8)

pca_very_strict_recipe <- base_recipe %>%
  step_pca(all_predictors(), threshold = 0.95)

kpca_poly_recipe <- base_recipe %>%
  step_kpca_poly(all_predictors(), num_comp = 10)

modified_base <- recipe(winner ~ ., data = momam_train) %>%
  step_rm(grouping) %>%
  step_mutate(cum_playtime_pie = log(cum_playtime_pie + 1)) %>%
  step_mutate(cum_playtime_spike = log(cum_playtime_spike + 1)) %>%
  step_zv(all_predictors()) %>%
  step_nzv(all_predictors(), freq_cut = 0.05, unique_cut = 0.01) %>%
  step_corr(all_predictors()) %>%
  step_lincomb(all_numeric_predictors()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
  
```


Define models:

These are the models is trained on. Currently a few simple models are here as a baseline,
but ideally more will be used.

```{r models}
model_dt <- decision_tree(tree_depth = tune(), min_n = tune(), cost_complexity = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  translate()

model_lr <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

model_lda <- discrim_linear() %>% 
  set_engine("sda") %>% 
  translate()

model_linear_mda <- discrim_linear(penalty = tune()) %>%
  set_engine("mda")

model_rf <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("randomForest") %>% 
  set_mode("classification") %>% 
  translate()

model_nb <- naive_Bayes(smoothness = tune(), Laplace = tune()) %>%
  set_engine("naivebayes") %>% 
  translate()

model_xgb <- boost_tree(tree_depth = tune(),
                        trees = tune(),
                        learn_rate = tune(),
                        mtry = tune(),
                        min_n = tune(),
                        loss_reduction = tune(),
                        sample_size = tune(),
                        stop_iter = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification") %>%
  translate()

model_c5_rules <- C5_rules(trees = tune(), min_n = tune()) %>%
  set_engine("C5.0") %>%
  set_mode("classification") %>%
  translate()

model_neighbor <- nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) %>%
  set_engine("kknn") %>% 
  set_mode("classification")

model_mlp <- mlp(
  hidden_units = tune(),
  penalty = tune(),
  epochs = tune()
) %>%  
  set_engine("nnet") %>% 
  set_mode("classification")
```


```{r workflow}
library(agua)
library(rules)

momam_workflowset <- workflow_set(
  preproc = list(base = base_recipe,
                 pca = pca_recipe,
                 strict = pca_strict_recipe,
                 very_strict = pca_very_strict_recipe,
                 kpca = kpca_poly_recipe,
                 modified = modified_base,
                 grouped_pca = grouped_pca,
                 hierarchical_pca = hierarchical_pca),
  models = list(
    decision_tree = model_dt,
    logistic_regression = model_lr,
    discrim_linear = model_lda,
    linear_mda = model_linear_mda,
    rand_forest = model_rf,
    naive_bayes = model_nb,
    xgboost = model_xgb,
    C5_Rules = model_c5_rules,
    nearest_neighbor = model_neighbor,
    mlp = model_mlp
  ),
  cross = TRUE
)
```

```{r train}
library(finetune)

race_ctrl <-
  control_race(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

library(doParallel)
doParallel::registerDoParallel(cores = 10)

grid_results <-
  momam_workflowset %>%
  workflow_map(
    "tune_race_anova",
    seed = 0323,
    resamples = momam_folds,
    grid = 15,
    control = race_ctrl,
    verbose = TRUE
  )

doParallel::stopImplicitCluster()
```

```{r evaluate}
autoplot(
  grid_results,
  rank_metric = "roc_auc",  # <- how to order models
  #metric = "accuracy",       # <- which metric to visualize
  select_best = TRUE     # <- one point per workflow
)
```

```{r}
grid_results %>% 
  workflowsets::rank_results(rank_metric = "roc_auc") %>%
  pivot_wider(names_from = .metric, values_from = mean) %>%
  group_by(.config) %>% 
  fill(accuracy, roc_auc, .direction = "downup") %>%
  distinct(rank, .keep_all = TRUE) %>%
  mutate(preprocessor = str_extract(wflow_id, ".*?(?=_)")) %>%
  select(wflow_id, model, preprocessor, accuracy, roc_auc, rank) %>%
  arrange(desc(roc_auc))
```

```{r}
best_results <- 
  grid_results %>% 
  extract_workflow_set_result("grouped_pca_rand_forest") %>% 
  select_best(metric = "roc_auc")

best_results
```
```{r}
best_test_results <- 
  grid_results %>% 
  extract_workflow("grouped_pca_rand_forest") %>% 
  finalize_workflow(best_results) %>% 
  last_fit(split = initial_split)
```

```{r}
grid_results %>% 
  extract_workflow_set_result("grouped_pca_C5_Rules")
```

```{r}
best_test_results$.metrics
```

```{r}
predictions <- best_test_results %>% 
  collect_predictions() 

predictions

predictions %>%
  mutate(correct = ifelse(.pred_class == winner, 1, 0)) %>%
  mutate(weighted_answer = case_when(winner == "pie" ~ .pred_pie,
                                     T ~ .pred_spike)) %>%
  summarise(test_acc = sum(correct)/nrow(.), test_roc = sum(weighted_answer)/nrow(.))
```